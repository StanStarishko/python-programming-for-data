{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.2 Working with Strings.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StanStarishko/python-programming-for-data/blob/main/Worksheets/2_2_Working_with_Strings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdT0fwTCa4gI"
      },
      "source": [
        "# Working with Strings\n",
        "---\n",
        "\n",
        "The pandas library has a similar set of string functions to those available in python generally.  Because we often want to perform operations on a whole series of data values in a dataframe, we can use pandas string functions to do this:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a series from a pandas column\n",
        "---\n",
        "\n",
        "A column of data is treated by pandas as a Series.  There is a set of functions that you can access for working with a **series** (just one column from the data table).\n",
        "\n",
        "To get a 'series' from a dataframe, you would split the column from the rest of the dataframe by taking a copy of it and storing it in a new variable (which is very similar to a list).\n",
        "\n",
        "The examples below show what you can do with a Series rather than a whole table.\n",
        "\n",
        "To get a column of data as a series:\n",
        "\n",
        "```\n",
        "series_data = df['date']\n",
        "```\n",
        "or\n",
        "```\n",
        "price_series = df['price']\n",
        "```\n",
        "where 'date' or 'price' are the names of the columns in the dataframe"
      ],
      "metadata": {
        "id": "GEi6lFzV2dAa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozWLM6Pfa7d_"
      },
      "source": [
        "### Splitting data\n",
        "---\n",
        "\n",
        "Series.str.split() *to split a column's strings into components*    \n",
        "Series.str.get() *to get one of the components after the split*  \n",
        "\n",
        "You can **daisychain** these together:   \n",
        "\n",
        "`Series.str.split().str.get()`\n",
        "\n",
        "* `split()` will split by white space unless specified, for example if you wanted to split by \"/\" you would use `split(\"/\")`  \n",
        "\n",
        "* `get()` requires a parameter of the value position of the string you would like to 'get'. If you want the first word eg 1999, use `get(0)`.\n",
        "\n",
        "\n",
        "*Hint: remember to save your result into a new column*\n",
        "\n",
        "### Exercise 1 strings\n",
        "---\n",
        "\n",
        "Let's use the data set 'Housing in London' at 'https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv'\n",
        "\n",
        "The date, in this dataset is a string.   To filter for a particular year, we will need to extract the first four letters as a substring.  We can create a new column called **year**, which just contains the year, stored as a number.\n",
        "\n",
        "The date is written in the format yyyy-mm-dd.  We can split the year around the '-' and then use the first component.\n",
        "\n",
        "\n",
        "Create a function called **get_year()** which splits the data from the date column, and creates a year column with just the year before returning the year column.\n",
        "\n",
        "\n",
        "**Test output**:  \n",
        "\n",
        "```\n",
        "0       1999\n",
        "1       1999\n",
        "2       1999\n",
        "3       1999\n",
        "4       1999\n",
        "        ...\n",
        "1066    2019\n",
        "1067    2019\n",
        "1068    2019\n",
        "1069    2019\n",
        "1070    2019\n",
        "Name: year, Length: 1071, dtype: object\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuLewd421t_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3fd6e9f-1114-474d-8a64-6bb513303f42"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def is_valid_link(link=\"\",link_name=\"\",autotest=False):\n",
        "  # link always isn't empty and must have is string\n",
        "  return_value = link != \"\" and isinstance(link, str)\n",
        "\n",
        "  if not return_value and not autotest: # not print if autotest\n",
        "    print(f\"{link_name} is not valid\")\n",
        "\n",
        "  return return_value\n",
        "\n",
        "\n",
        "def get_year(url=\"\"):\n",
        "  # add code below to return a new series created from the year column, with just years\n",
        "\n",
        "  if not is_valid_link(url,\"url\"):\n",
        "    return False\n",
        "\n",
        "  df = pd.read_csv(url)\n",
        "\n",
        "  # create column \"year\" from full-date\n",
        "  df[\"year\"] = df[\"date\"].str.split('-').str.get(0)\n",
        "  print(df[\"year\"])\n",
        "\n",
        "  return df[\"year\"]\n",
        "\n",
        "\n",
        "\n",
        "# run and test if returned series is of correct length and has correct first row\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv\"\n",
        "\n",
        "series_year = get_year(url)\n",
        "# actual_len = len(get_year(url))\n",
        "# actual_value = get_year(url).iloc[0]\n",
        "actual_len = len(series_year)\n",
        "actual_value = series_year.iloc[0]\n",
        "expected_len = 1071\n",
        "expected_val = \"1999\"\n",
        "\n",
        "if actual_len == expected_len and actual_value == expected_val:\n",
        "  print(\"Test passed expected length 1071 and first value 1999 and got\", actual_len, actual_value)\n",
        "else:\n",
        "  print(\"Test failed expected length 1071 and first value 1999 and got length\", actual_len, \"value\", actual_value)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       1999\n",
            "1       1999\n",
            "2       1999\n",
            "3       1999\n",
            "4       1999\n",
            "        ... \n",
            "1066    2019\n",
            "1067    2019\n",
            "1068    2019\n",
            "1069    2019\n",
            "1070    2019\n",
            "Name: year, Length: 1071, dtype: object\n",
            "Test passed expected length 1071 and first value 1999 and got 1071 1999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPAgjR5U1utf"
      },
      "source": [
        "### Exercise 2\n",
        "---\n",
        "\n",
        "In exercise 1 you have extracted the year, but it's dtype is 'object' (it is still a string).  You can convert to integer by adding  .astype(int) to the daisychain.\n",
        "\n",
        "Create a new function called **get_int_year()**, `return` the year column with values of type int.\n",
        "\n",
        "**Test output**:  \n",
        "\n",
        "```\n",
        "...\n",
        "Name: year, Length: 1071, dtype: int64\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SuKrrvD2f1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3851c5c4-df1d-4ea2-9854-0defac00d7b8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_int_year(series_in=None):\n",
        "  # add code below to return a year column where values are of integer type\n",
        "\n",
        "  if series_in is None:\n",
        "    return False\n",
        "\n",
        "  # create column \"year\" from str to int\n",
        "  series_out = series_in.astype(np.int64)\n",
        "  print(series_out)\n",
        "\n",
        "  return series_out\n",
        "\n",
        "\n",
        "# run and test if your returned series is of type int\n",
        "\n",
        "actual = get_int_year(series_year).dtype\n",
        "expected = np.int64\n",
        "\n",
        "if actual == expected:\n",
        "  print(\"Test passed\", actual)\n",
        "else:\n",
        "  print(\"Test failed, expected\", expected, \"got\", actual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       1999\n",
            "1       1999\n",
            "2       1999\n",
            "3       1999\n",
            "4       1999\n",
            "        ... \n",
            "1066    2019\n",
            "1067    2019\n",
            "1068    2019\n",
            "1069    2019\n",
            "1070    2019\n",
            "Name: year, Length: 1071, dtype: int64\n",
            "Test passed int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spoUBwxT2gSi"
      },
      "source": [
        "### Exercise 3\n",
        "---\n",
        "\n",
        "All the areas in the data set are in lower case.  To prepare the data for reporting, you may want to capitalise.  Use .str.title() to do this.\n",
        "\n",
        "Create the function **get_title_areas()** to do this. `Return` the newly capitalised area column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fLUjUYk4AyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd43868d-3c11-4f1e-e210-ad38a1845805"
      },
      "source": [
        "def get_title_areas(url=\"\",print_df=True):\n",
        "  # add code below to capitalise the first letter of each string in the column 'area'\n",
        "\n",
        "  if not is_valid_link(url,\"url\"):\n",
        "    return False\n",
        "\n",
        "  df = pd.read_csv(url)\n",
        "\n",
        "  # create column \"year\" from full-date\n",
        "  df[\"area\"] = df[\"area\"].str.title()\n",
        "\n",
        "  if print_df:\n",
        "    print(df[\"area\"])\n",
        "\n",
        "  return df[\"area\"]\n",
        "\n",
        "\n",
        "# run and test if the first row of the area column is now correct\n",
        "\n",
        "actual = get_title_areas(url).iloc[0]\n",
        "expected = \"City Of London\"\n",
        "\n",
        "if actual == expected:\n",
        "  print(\"Test passed\", actual)\n",
        "else:\n",
        "  print(\"Test failed, expected\", expected, \"got\", actual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0             City Of London\n",
            "1       Barking And Dagenham\n",
            "2                     Barnet\n",
            "3                     Bexley\n",
            "4                      Brent\n",
            "                ...         \n",
            "1066           Great Britain\n",
            "1067       England And Wales\n",
            "1068        Northern Ireland\n",
            "1069                Scotland\n",
            "1070                   Wales\n",
            "Name: area, Length: 1071, dtype: object\n",
            "Test passed City Of London\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuUQQF2a4CPX"
      },
      "source": [
        "### Exercise 4 - Filter all areas to find all with 'And' in the name\n",
        "---\n",
        "\n",
        "Create a function called **get_and()** which uses `str.contains()` and a search (e.g. df[df['area'].str.contains()]) to filter and `return` all areas with 'And' in the name  (Note:  case is important)\n",
        "\n",
        "**Test output**:  \n",
        "105 rows × 13 columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmT32YMq4BA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40da307-a780-4cb7-8aa5-d9ed5a772c81"
      },
      "source": [
        "def get_and(url=\"\"):\n",
        "# add code to return just rows in which area contains 'And'\n",
        "\n",
        "  if not is_valid_link(url,\"url\"):\n",
        "    return False\n",
        "\n",
        "  df = pd.read_csv(url)\n",
        "\n",
        "  contains_df = df[df[\"area\"].str.contains(\" and \")]\n",
        "  print(contains_df)\n",
        "\n",
        "  return contains_df\n",
        "\n",
        "\n",
        "# run and test if returned is correct length\n",
        "\n",
        "actual = len(get_and(url))\n",
        "expected = 105\n",
        "\n",
        "if actual == expected:\n",
        "  print(\"Test passed\", actual)\n",
        "else:\n",
        "  print(\"Test failed, expected\", expected, \"got\", actual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           code                      area        date  median_salary  \\\n",
            "1     E09000002      barking and dagenham  1999-12-01        21480.0   \n",
            "12    E09000013    hammersmith and fulham  1999-12-01        25000.0   \n",
            "19    E09000020    kensington and chelsea  1999-12-01        20646.0   \n",
            "35    E12000003  yorkshire and the humber  1999-12-01        16527.0   \n",
            "47    K04000001         england and wales  1999-12-01        17974.0   \n",
            "...         ...                       ...         ...            ...   \n",
            "1021  E09000002      barking and dagenham  2019-12-01        28738.0   \n",
            "1032  E09000013    hammersmith and fulham  2019-12-01        37990.0   \n",
            "1039  E09000020    kensington and chelsea  2019-12-01        33000.0   \n",
            "1055  E12000003  yorkshire and the humber  2019-12-01        27835.0   \n",
            "1067  K04000001         england and wales  2019-12-01        30500.0   \n",
            "\n",
            "      life_satisfaction mean_salary recycling_pct  population_size  \\\n",
            "1                   NaN       23620             3         162444.0   \n",
            "12                  NaN       28555             7         160634.0   \n",
            "19                  NaN       28074            13         147678.0   \n",
            "35                  NaN       18977             7        4956325.0   \n",
            "47                  NaN       21549           NaN       51933471.0   \n",
            "...                 ...         ...           ...              ...   \n",
            "1021                NaN       32010           NaN              NaN   \n",
            "1032                NaN       48362           NaN              NaN   \n",
            "1039                NaN       41741           NaN              NaN   \n",
            "1055                NaN       32653           NaN              NaN   \n",
            "1067                NaN       37865           NaN              NaN   \n",
            "\n",
            "      number_of_jobs  area_size  no_of_houses  borough_flag  \n",
            "1                NaN        NaN           NaN             1  \n",
            "12               NaN        NaN           NaN             1  \n",
            "19               NaN        NaN           NaN             1  \n",
            "35               NaN        NaN           NaN             0  \n",
            "47               NaN        NaN           NaN             0  \n",
            "...              ...        ...           ...           ...  \n",
            "1021             NaN        NaN           NaN             1  \n",
            "1032             NaN        NaN           NaN             1  \n",
            "1039             NaN        NaN           NaN             1  \n",
            "1055             NaN        NaN           NaN             0  \n",
            "1067             NaN        NaN           NaN             0  \n",
            "\n",
            "[105 rows x 12 columns]\n",
            "Test passed 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ChmyffT7wDK"
      },
      "source": [
        "### Exercise 5\n",
        "---\n",
        "\n",
        "Filter the data for all areas starting with 'Ba'  \n",
        "\n",
        "*hint: use `startswith()`*\n",
        "\n",
        "**Test Ouput:**  \n",
        "42 rows, first row has area 'Barking and Dagenham'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li8sAN3O8Zxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdcd5963-1e22-4e80-8f85-b6059f7e7ccc"
      },
      "source": [
        "def get_ba(url=\"\"):\n",
        "  # add code to filter for all areas starting with 'Ba'\n",
        "\n",
        "  if not is_valid_link(url,\"url\"):\n",
        "    return False\n",
        "\n",
        "  df = get_title_areas(url,print_df=False)\n",
        "\n",
        "  contains_df = df[df.str.startswith(\"Ba\")]\n",
        "  print(contains_df)\n",
        "\n",
        "  return contains_df\n",
        "\n",
        "\n",
        "# run and test if your returned rows are the right length\n",
        "\n",
        "actual = len(get_ba(url))\n",
        "expected = 42\n",
        "\n",
        "if actual == expected:\n",
        "  print(\"Test passed\", actual)\n",
        "else:\n",
        "  print(\"Test failed, expected\", expected, \"got\", actual)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1       Barking And Dagenham\n",
            "2                     Barnet\n",
            "52      Barking And Dagenham\n",
            "53                    Barnet\n",
            "103     Barking And Dagenham\n",
            "104                   Barnet\n",
            "154     Barking And Dagenham\n",
            "155                   Barnet\n",
            "205     Barking And Dagenham\n",
            "206                   Barnet\n",
            "256     Barking And Dagenham\n",
            "257                   Barnet\n",
            "307     Barking And Dagenham\n",
            "308                   Barnet\n",
            "358     Barking And Dagenham\n",
            "359                   Barnet\n",
            "409     Barking And Dagenham\n",
            "410                   Barnet\n",
            "460     Barking And Dagenham\n",
            "461                   Barnet\n",
            "511     Barking And Dagenham\n",
            "512                   Barnet\n",
            "562     Barking And Dagenham\n",
            "563                   Barnet\n",
            "613     Barking And Dagenham\n",
            "614                   Barnet\n",
            "664     Barking And Dagenham\n",
            "665                   Barnet\n",
            "715     Barking And Dagenham\n",
            "716                   Barnet\n",
            "766     Barking And Dagenham\n",
            "767                   Barnet\n",
            "817     Barking And Dagenham\n",
            "818                   Barnet\n",
            "868     Barking And Dagenham\n",
            "869                   Barnet\n",
            "919     Barking And Dagenham\n",
            "920                   Barnet\n",
            "970     Barking And Dagenham\n",
            "971                   Barnet\n",
            "1021    Barking And Dagenham\n",
            "1022                  Barnet\n",
            "Name: area, dtype: object\n",
            "Test passed 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrSsOstK8Z9_"
      },
      "source": [
        "### Exercise 6\n",
        "---\n",
        "Create function called **get_ham()** to filter and `return` the data for all areas ending with 'ham', for the year 2000\n",
        "\n",
        "*hint: use `endswith()`*   \n",
        "\n",
        "**Test output**:  \n",
        "4 rows (barking and dagenham, hammersmith and fulham, lewisham, newham)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMJckNo-ab3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a40cf5-ef4b-42c1-b733-982b4fbd539c"
      },
      "source": [
        "\n",
        "def get_ham(url=\"\"):\n",
        "  # add code to return rows which end with 'ham' for the year 2000\n",
        "\n",
        "  if not is_valid_link(url,\"url\"):\n",
        "    return False\n",
        "\n",
        "  df = pd.read_csv(url)\n",
        "\n",
        "  # filtered with year equals 2000\n",
        "  df[\"year\"] = df[\"date\"].str.split('-').str.get(0)\n",
        "  df_year = df[df[\"year\"] == \"2000\"]\n",
        "\n",
        "  contains_df = df_year[df_year[\"area\"].str.endswith(\"ham\")][\"area\"]\n",
        "  print(contains_df)\n",
        "\n",
        "  return contains_df\n",
        "\n",
        "\n",
        "\n",
        "# run and test if correct number of rows are returned\n",
        "\n",
        "actual = len(get_ham(url))\n",
        "expected = 4\n",
        "\n",
        "if actual == expected:\n",
        "  print(\"Test passed\", actual)\n",
        "else:\n",
        "  print(\"Test failed, expected\", expected, \"got\", actual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52      barking and dagenham\n",
            "63    hammersmith and fulham\n",
            "73                  lewisham\n",
            "75                    newham\n",
            "Name: area, dtype: object\n",
            "Test passed 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_niKyF39X4q"
      },
      "source": [
        "### Exercise 7 - new data set\n",
        "---\n",
        "\n",
        "Use the data set here:  https://github.com/futureCodersSE/working-with-data/blob/main/Data%20sets/public_use-talent-migration.xlsx?raw=true\n",
        "\n",
        "Read the data from the sheet 'Skill Migration'  \n",
        "\n",
        "Write a function called **create_new_df()** which will inspect the data, then create and return new dataframe with the following changes:\n",
        "\n",
        "1.  Remove the word 'Skills' from the 'skill_group_category' column   \n",
        "  *hint: you can use the `str.rstrip()` function*\n",
        "2.  Convert country_code to uppercase    \n",
        "  *hint: try `upper()`*  \n",
        "4.  Remove the skill_group_id and the wb_income columns\n",
        "3.  Filter for regions containing 'Asia'  \n",
        "  *hint: you might have to `return` it*\n",
        "\n",
        "**Test output**:  \n",
        "9969 rows × 10 columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGfVBX1FCjZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ecdeab-13bc-4772-ea8e-5e487c55fb18"
      },
      "source": [
        "\n",
        "def get_excel_data(url=\"\",sheet_name=\"default\"):\n",
        "  # url and sheet name always isn't empty and must have is string\n",
        "  is_not_valid_url = not is_valid_link(url,\"url\")\n",
        "  is_not_valid_sheet_name = not is_valid_link(sheet_name,\"sheet name\")\n",
        "  if is_not_valid_url or is_not_valid_sheet_name:\n",
        "    return False\n",
        "\n",
        "  if sheet_name == \"default\":\n",
        "    df = pd.read_excel(url)\n",
        "  else:\n",
        "    df = pd.read_excel(url,sheet_name)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def create_new_df(url=\"\"):\n",
        "  # add code below to return a df with 'Skills' removed, country_code in uppercase, no skill_group_id or wb_income columns and only for regions containing Asia\n",
        "\n",
        "  if not is_valid_link(url,\"url\"):\n",
        "    return False\n",
        "\n",
        "  #get data frame\n",
        "  df = get_excel_data(url,sheet_name=\"Skill Migration\")\n",
        "\n",
        "  # 1. remove 'Skills' from the 'skill_group_category' column\n",
        "  df[\"skill_group_category\"] = df[\"skill_group_category\"].str.rstrip(\"Skills\")\n",
        "\n",
        "  # 2. Convert country_code to uppercase\n",
        "  df[\"country_code\"] = df[\"country_code\"].str.upper()\n",
        "\n",
        "  # 3. Remove the skill_group_id and the wb_income columns\n",
        "  df = df.drop(columns=[\"skill_group_id\",\"wb_income\"])\n",
        "\n",
        "  # 4. Filter for regions containing 'Asia'\n",
        "  df = df[df[\"wb_region\"].str.contains(\"Asia\")]\n",
        "\n",
        "  print(df)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "# run and test if returned dataframe is correct length, with the right number of columns  and first row skill_group_category is correct\n",
        "url = \"https://github.com/futureCodersSE/working-with-data/blob/main/Data%20sets/public_use-talent-migration.xlsx?raw=true\"\n",
        "test_df = create_new_df(url)\n",
        "actual_len = len(test_df)\n",
        "actual_col = len(test_df.columns)\n",
        "expected_len = 9969\n",
        "expected_col = 10\n",
        "actual_skill = test_df['skill_group_category'].iloc[0]\n",
        "expected_skill = 'Tech '\n",
        "\n",
        "if actual_len == expected_len and actual_col == expected_col and actual_skill == expected_skill:\n",
        "  print(\"Test passed\", actual_len, \"x\", actual_col, actual_skill)\n",
        "else:\n",
        "  print(\"Test failed, expected\", expected_len, \"x\", expected_col, expected_skill, \"got\", actual_len, \"x\", actual_col, actual_skill)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      country_code country_name            wb_region   skill_group_category  \\\n",
            "0               AF  Afghanistan           South Asia                  Tech    \n",
            "1               AF  Afghanistan           South Asia              Business    \n",
            "2               AF  Afghanistan           South Asia  Specialized Industry    \n",
            "3               AF  Afghanistan           South Asia                  Tech    \n",
            "4               AF  Afghanistan           South Asia  Specialized Industry    \n",
            "...            ...          ...                  ...                    ...   \n",
            "17371           VN      Vietnam  East Asia & Pacific  Specialized Industry    \n",
            "17372           VN      Vietnam  East Asia & Pacific  Specialized Industry    \n",
            "17373           VN      Vietnam  East Asia & Pacific              Business    \n",
            "17374           VN      Vietnam  East Asia & Pacific       Disruptive Tech    \n",
            "17375           VN      Vietnam  East Asia & Pacific              Business    \n",
            "\n",
            "             skill_group_name  net_per_10K_2015  net_per_10K_2016  \\\n",
            "0      Information Management           -791.59           -705.88   \n",
            "1      Operational Efficiency          -1610.25           -933.55   \n",
            "2           National Security          -1731.45           -769.68   \n",
            "3            Software Testing           -957.50           -828.54   \n",
            "4                        Navy          -1510.71           -841.17   \n",
            "...                       ...               ...               ...   \n",
            "17371               Air Force            658.89            291.89   \n",
            "17372                 Cooking             83.81            236.02   \n",
            "17373             Sales Leads            511.16             98.13   \n",
            "17374   Aerospace Engineering            410.02             42.71   \n",
            "17375        Revenue Analysis            315.68            158.12   \n",
            "\n",
            "       net_per_10K_2017  net_per_10K_2018  net_per_10K_2019  \n",
            "0               -550.04           -680.92          -1208.79  \n",
            "1               -776.06           -532.22           -790.09  \n",
            "2               -756.59           -600.44           -767.64  \n",
            "3               -964.73           -406.50           -739.51  \n",
            "4               -842.32           -581.71           -718.64  \n",
            "...                 ...               ...               ...  \n",
            "17371           -271.00            219.20            166.99  \n",
            "17372            -42.85            238.90            180.90  \n",
            "17373           -355.91            133.89            181.47  \n",
            "17374           -222.09            138.03            256.55  \n",
            "17375            101.89            277.98            267.58  \n",
            "\n",
            "[9969 rows x 10 columns]\n",
            "Test passed 9969 x 10 Tech \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_i4oQaoEKoY"
      },
      "source": [
        "### Exercise 8\n",
        "---\n",
        "\n",
        "Write a function called **clean_skills()** that will:\n",
        "1. rename the **net_per_10K_year** columns to be just the year\n",
        "2. in the **skill_group_category** column replace the 'z' in 'specialized' with 's' to Anglicise the spelling.\n",
        "\n",
        "The function should `return` the cleaned data.  \n",
        "\n",
        "Hint:  You can use the `replace()` function to replace substring's and characters in both column headings and the actual data.  \n",
        "* `.str.replace(\"old\",\"new\")`\n",
        "\n",
        "**Test output**:  \n",
        "17617 rows × 12 columns, with z replace by s in Specialized  \n",
        "Column names: country_code\tcountry_name\twb_income\twb_region\tskill_group_id\tskill_group_category\tskill_group_name\t2015\t2016\t2017\t2018\t2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEWS56l4JKx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d546bc-9af2-44fb-aea2-d6dfca2822f5"
      },
      "source": [
        "#migration = # add code to open dataset here\n",
        "migration = get_excel_data(url,sheet_name=\"Skill Migration\")\n",
        "\n",
        "def clean_skills(df):\n",
        "  # add code to clean the data\n",
        "  # rename the net_per_10K_year columns to be just the year\n",
        "  migration.columns = migration.columns.str.replace(\"net_per_10K_\",\"\")\n",
        "\n",
        "  # in the skill_group_category column replace the 'z' in 'specialized' with 's' to Anglicise the spelling\n",
        "  migration[\"skill_group_category\"] = migration[\"skill_group_category\"].str.replace(\"pecialized\",\"pecialised\")\n",
        "\n",
        "  return migration\n",
        "\n",
        "\n",
        "\n",
        "  # in the skill_group_category column replace the 'z' in 'specialized' with 's' to Anglicise the spelling\n",
        "\n",
        "  # return the cleaned data\n",
        "\n",
        "\n",
        "# run and test if columns have correct names and specialised is anglised\n",
        "test_df = clean_skills(migration)\n",
        "\n",
        "if (test_df['skill_group_category'].str.contains('Specialised').any() == True) and (test_df.columns.str.contains('net_per_10K_').any() == False):\n",
        "  print(\"Test passed\")\n",
        "else:\n",
        "  print(\"Test failed\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqc9I5VpJLpR"
      },
      "source": [
        "### Exercise 9\n",
        "---\n",
        "\n",
        "Read the 'Country Migration' sheet.\n",
        "\n",
        "Write a function that will:  \n",
        "*  convert the country codes to upper case  \n",
        "*  drop the lat and long columns for both base and target  \n",
        "*  rename the net_per_10K_year columns to year only  \n",
        "*  filter for base_country_wb_region contains 'Africa' and target_country_wb_region contains Asia  \n",
        "\n",
        "**Test output**:  \n",
        "```\n",
        "base_country_code\tbase_country_name\tbase_country_wb_income\tbase_country_wb_region\ttarget_country_code\ttarget_country_name\ttarget_country_wb_income\ttarget_country_wb_region\t2015\t2016\t2017\t2018\t2019\n",
        "0\tAE\tUnited Arab Emirates\tHigh Income\tMiddle East & North Africa\tAF\tAfghanistan\tLow Income\tSouth Asia\t0.19\t0.16\t0.11\t-0.05\t-0.02\n",
        "4\tAE\tUnited Arab Emirates\tHigh Income\tMiddle East & North Africa\tAM\tArmenia\tUpper Middle Income\tEurope & Central Asia\t0.10\t0.05\t0.03\t-0.01\t0.02\n",
        "5\tAE\tUnited Arab Emirates\tHigh Income\tMiddle East & North Africa\tAU\tAustralia\tHigh Income\tEast Asia & Pacific\t-1.06\t-3.31\t-4.01\t-4.58\t-4.09\n",
        "6\tAE\tUnited Arab Emirates\tHigh Income\tMiddle East & North Africa\tAT\tAustria\tHigh Income\tEurope & Central Asia\t0.11\t-0.08\t-0.07\t-0.05\t-0.16\n",
        "7\tAE\tUnited Arab Emirates\tHigh Income\tMiddle East & North Africa\tAZ\tAzerbaijan\tUpper Middle Income\tEurope & Central Asia\t0.24\t0.25\t0.10\t0.05\t0.04\n",
        "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
        "4132\tZM\tZambia\tLower Middle Income\tSub-Saharan Africa\tGB\tUnited Kingdom\tHigh Income\tEurope & Central Asia\t43.27\t27.60\t7.88\t6.90\t3.68\n",
        "4135\tZW\tZimbabwe\tLow Income\tSub-Saharan Africa\tAU\tAustralia\tHigh Income\tEast Asia & Pacific\t-1.31\t-2.33\t-2.10\t-2.08\t-1.84\n",
        "4138\tZW\tZimbabwe\tLow Income\tSub-Saharan Africa\tIS\tIceland\tHigh Income\tEurope & Central Asia\t8.52\t6.22\t2.35\t1.81\t0.97\n",
        "4142\tZW\tZimbabwe\tLow Income\tSub-Saharan Africa\tNO\tNorway\tHigh Income\tEurope & Central Asia\t2.88\t6.46\t2.10\t0.33\t-0.13\n",
        "4145\tZW\tZimbabwe\tLow Income\tSub-Saharan Africa\tGB\tUnited Kingdom\tHigh Income\tEurope & Central Asia\t3.91\t4.66\t0.74\t-0.66\t-1.97\n",
        "478 rows × 13 columns\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYu6n_jF9v1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6eda52-e2d2-422b-98a0-7da293408d67"
      },
      "source": [
        "def clean_country_mig(url=\"\"):\n",
        "  # add code below to clean the data\n",
        "  df = get_excel_data(url,sheet_name=\"Country Migration\")\n",
        "\n",
        "  # convert the country codes to upper case\n",
        "  df[\"base_country_code\"] = df[\"base_country_code\"].str.upper()\n",
        "  df[\"target_country_code\"] = df[\"target_country_code\"].str.upper()\n",
        "\n",
        "  # drop the lat and long columns for both base and target\n",
        "  df = df.drop(columns=[\"base_lat\",\"base_long\",\"target_lat\",\"target_long\"])\n",
        "\n",
        "  # rename the net_per_10K_year columns to year only\n",
        "  df.columns = df.columns.str.replace(\"net_per_10K_\",\"\")\n",
        "\n",
        "  # filter for base_country_wb_region contains 'Africa' and target_country_wb_region contains Asia\n",
        "  df = df[df[\"base_country_wb_region\"].str.contains(\"Africa\")]\n",
        "  df = df[df[\"target_country_wb_region\"].str.contains(\"Asia\")]\n",
        "\n",
        "  print(df)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# run test if there is the correct number of columns, country codes are in uppercase and year columns have been reformatted\n",
        "\n",
        "#test_df = clean_country_mig(url)\n",
        "df = clean_country_mig(url)\n",
        "#actual_col_len = len(test_df.columns)\n",
        "actual_col_len = len(df.columns)\n",
        "expected = 13\n",
        "\n",
        "if actual_col_len == expected and (df['base_country_code'].str.islower().any() == False) and (df.columns.str.contains('net_per_10K_').any() == False):\n",
        "  print(\"Test passed\")\n",
        "else:\n",
        "  print(\"Test failed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     base_country_code     base_country_name base_country_wb_income  \\\n",
            "0                   AE  United Arab Emirates            High Income   \n",
            "4                   AE  United Arab Emirates            High Income   \n",
            "5                   AE  United Arab Emirates            High Income   \n",
            "6                   AE  United Arab Emirates            High Income   \n",
            "7                   AE  United Arab Emirates            High Income   \n",
            "...                ...                   ...                    ...   \n",
            "4132                ZM                Zambia    Lower Middle Income   \n",
            "4135                ZW              Zimbabwe             Low Income   \n",
            "4138                ZW              Zimbabwe             Low Income   \n",
            "4142                ZW              Zimbabwe             Low Income   \n",
            "4145                ZW              Zimbabwe             Low Income   \n",
            "\n",
            "          base_country_wb_region target_country_code target_country_name  \\\n",
            "0     Middle East & North Africa                  AF         Afghanistan   \n",
            "4     Middle East & North Africa                  AM             Armenia   \n",
            "5     Middle East & North Africa                  AU           Australia   \n",
            "6     Middle East & North Africa                  AT             Austria   \n",
            "7     Middle East & North Africa                  AZ          Azerbaijan   \n",
            "...                          ...                 ...                 ...   \n",
            "4132          Sub-Saharan Africa                  GB      United Kingdom   \n",
            "4135          Sub-Saharan Africa                  AU           Australia   \n",
            "4138          Sub-Saharan Africa                  IS             Iceland   \n",
            "4142          Sub-Saharan Africa                  NO              Norway   \n",
            "4145          Sub-Saharan Africa                  GB      United Kingdom   \n",
            "\n",
            "     target_country_wb_income target_country_wb_region   2015   2016  2017  \\\n",
            "0                  Low Income               South Asia   0.19   0.16  0.11   \n",
            "4         Upper Middle Income    Europe & Central Asia   0.10   0.05  0.03   \n",
            "5                 High Income      East Asia & Pacific  -1.06  -3.31 -4.01   \n",
            "6                 High Income    Europe & Central Asia   0.11  -0.08 -0.07   \n",
            "7         Upper Middle Income    Europe & Central Asia   0.24   0.25  0.10   \n",
            "...                       ...                      ...    ...    ...   ...   \n",
            "4132              High Income    Europe & Central Asia  43.27  27.60  7.88   \n",
            "4135              High Income      East Asia & Pacific  -1.31  -2.33 -2.10   \n",
            "4138              High Income    Europe & Central Asia   8.52   6.22  2.35   \n",
            "4142              High Income    Europe & Central Asia   2.88   6.46  2.10   \n",
            "4145              High Income    Europe & Central Asia   3.91   4.66  0.74   \n",
            "\n",
            "      2018  2019  \n",
            "0    -0.05 -0.02  \n",
            "4    -0.01  0.02  \n",
            "5    -4.58 -4.09  \n",
            "6    -0.05 -0.16  \n",
            "7     0.05  0.04  \n",
            "...    ...   ...  \n",
            "4132  6.90  3.68  \n",
            "4135 -2.08 -1.84  \n",
            "4138  1.81  0.97  \n",
            "4142  0.33 -0.13  \n",
            "4145 -0.66 -1.97  \n",
            "\n",
            "[478 rows x 13 columns]\n",
            "Test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V6mNsfsNrsd"
      },
      "source": [
        "### Exercise 10\n",
        "---\n",
        "\n",
        "Read the data from file 'https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/data/titanic.csv'.\n",
        "\n",
        "Write a function that will return a new dataframe with just the married women listed, surname only.\n",
        "\n",
        "**Test output**:  \n",
        "```\n",
        "\tPassengerId\tSurvived\tPclass\tName\tSex\tAge\tSibSp\tParch\tTicket\tFare\tCabin\tEmbarked\n",
        "1\t2\t1\t1\tCumings\tfemale\t38.0\t1\t0\tPC 17599\t71.2833\tC85\tC\n",
        "3\t4\t1\t1\tFutrelle\tfemale\t35.0\t1\t0\t113803\t53.1000\tC123\tS\n",
        "8\t9\t1\t3\tJohnson\tfemale\t27.0\t0\t2\t347742\t11.1333\tNaN\tS\n",
        "9\t10\t1\t2\tNasser\tfemale\t14.0\t1\t0\t237736\t30.0708\tNaN\tC\n",
        "15\t16\t1\t2\tHewlett\tfemale\t55.0\t0\t0\t248706\t16.0000\tNaN\tS\n",
        "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
        "871\t872\t1\t1\tBeckwith\tfemale\t47.0\t1\t1\t11751\t52.5542\tD35\tS\n",
        "874\t875\t1\t2\tAbelson\tfemale\t28.0\t1\t0\tP/PP 3381\t24.0000\tNaN\tC\n",
        "879\t880\t1\t1\tPotter\tfemale\t56.0\t0\t1\t11767\t83.1583\tC50\tC\n",
        "880\t881\t1\t2\tShelley\tfemale\t25.0\t0\t1\t230433\t26.0000\tNaN\tS\n",
        "885\t886\t0\t3\tRice\tfemale\t39.0\t0\t5\t382652\t29.1250\tNaN\tQ\n",
        "129 rows × 12 columns\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4gx3RHIOczI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50da0d2c-44ad-424c-a2bc-d449e84772d1"
      },
      "source": [
        "def get_married(url=\"\"):\n",
        "  # add code to return only the last names of married women\n",
        "\n",
        "  if not is_valid_link(url,\"url\"):\n",
        "    return False\n",
        "\n",
        "  df = pd.read_csv(url)\n",
        "\n",
        "  # filter for married women\n",
        "  df = df[df[\"Name\"].str.contains(\"Mrs.\")]  #df[df[\"Sex\"] == \"female\"]\n",
        "  #df = df[df[\"Age\"] >= 18]\n",
        "\n",
        "  # get last name\n",
        "  df[\"Name\"] = df[\"Name\"].str.split(\",\").str.get(0)\n",
        "\n",
        "  print(df)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# run and test if returned dataframe is correct length and has correct first row\n",
        "titanic = \"https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/data/titanic.csv\"\n",
        "test_df = get_married(titanic)\n",
        "actual_len = len(test_df)\n",
        "expected_len = 129\n",
        "actual_name = test_df['Name'].iloc[0]\n",
        "expected_name = 'Cumings'\n",
        "\n",
        "if actual_len == expected_len and actual_name == expected_name:\n",
        "  print(\"Test passed, \", actual_len, actual_name)\n",
        "else:\n",
        "  print(\"Test failed expected \", expected_len, expected_name, \"got\", actual_len, actual_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     PassengerId  Survived  Pclass      Name     Sex   Age  SibSp  Parch  \\\n",
            "1              2         1       1   Cumings  female  38.0      1      0   \n",
            "3              4         1       1  Futrelle  female  35.0      1      0   \n",
            "8              9         1       3   Johnson  female  27.0      0      2   \n",
            "9             10         1       2    Nasser  female  14.0      1      0   \n",
            "15            16         1       2   Hewlett  female  55.0      0      0   \n",
            "..           ...       ...     ...       ...     ...   ...    ...    ...   \n",
            "871          872         1       1  Beckwith  female  47.0      1      1   \n",
            "874          875         1       2   Abelson  female  28.0      1      0   \n",
            "879          880         1       1    Potter  female  56.0      0      1   \n",
            "880          881         1       2   Shelley  female  25.0      0      1   \n",
            "885          886         0       3      Rice  female  39.0      0      5   \n",
            "\n",
            "        Ticket     Fare Cabin Embarked  \n",
            "1     PC 17599  71.2833   C85        C  \n",
            "3       113803  53.1000  C123        S  \n",
            "8       347742  11.1333   NaN        S  \n",
            "9       237736  30.0708   NaN        C  \n",
            "15      248706  16.0000   NaN        S  \n",
            "..         ...      ...   ...      ...  \n",
            "871      11751  52.5542   D35        S  \n",
            "874  P/PP 3381  24.0000   NaN        C  \n",
            "879      11767  83.1583   C50        C  \n",
            "880     230433  26.0000   NaN        S  \n",
            "885     382652  29.1250   NaN        Q  \n",
            "\n",
            "[129 rows x 12 columns]\n",
            "Test passed,  129 Cumings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Rhwx2Sb1Ey"
      },
      "source": [
        "---\n",
        "### Optional extra practice\n",
        "\n",
        "There are some similar and some more challenging exercises [here](https://www.w3resource.com/python-exercises/date-time-exercise/) if you would like to practice more. The site has its own editor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQV2NO8umBSk"
      },
      "source": [
        "# Reflection\n",
        "----\n",
        "\n",
        "## What skills have you demonstrated in completing this notebook?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUVvt2r0mCKq"
      },
      "source": [
        "Your answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- attention to detail\n",
        "- scaling my own developments\n",
        "- applying my own development strategies\n",
        "- rapid acquisition of new knowledge"
      ],
      "metadata": {
        "id": "V3bEIY9zhxYb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOrbWOuFmObq"
      },
      "source": [
        "## What caused you the most difficulty?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_eGng1GmO78"
      },
      "source": [
        "Your answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I didn't really have any particular problems"
      ],
      "metadata": {
        "id": "jP5VwaRviF8g"
      }
    }
  ]
}